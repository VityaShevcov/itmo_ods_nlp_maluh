import numpy as np
import pandas as pd
from typing import List, Tuple, Dict, Optional, Union
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer
import pickle


class BERTopicModel:
    """–∫–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å BERTopic –º–æ–¥–µ–ª—å—é"""
    
    def __init__(self, 
                 embedding_model: str = "paraphrase-multilingual-MiniLM-L12-v2",
                 random_state: int = 42):
        self.random_state = random_state
        self.embedding_model_name = embedding_model
        self.model = None
        self.embeddings = None
        self.documents = None
        
        # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self._setup_components()
    
    def _setup_components(self):
        """–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ BERTopic"""
        # –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.sentence_model = SentenceTransformer(self.embedding_model_name)
        
        # —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        self.umap_model = UMAP(
            n_neighbors=15,
            n_components=5,
            min_dist=0.0,
            metric='cosine',
            random_state=self.random_state
        )
        
        # –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±—É–¥—É—Ç –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å—Å—è)
        self.hdbscan_model = HDBSCAN(
            min_cluster_size=15,
            metric='euclidean',
            cluster_selection_method='eom',
            prediction_data=True
        )
        
        # –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è c-TF-IDF
        self.vectorizer_model = CountVectorizer(
            ngram_range=(1, 2),
            stop_words=None,  # —É–∂–µ —É–¥–∞–ª–∏–ª–∏ –≤ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ
            min_df=2
        )
    
    def train(self, documents: List[str], 
              min_cluster_size: int = 15,
              nr_topics: Optional[Union[int, str]] = None,
              calculate_probabilities: bool = True,
              embeddings: Optional[np.ndarray] = None) -> None:
        """–æ–±—É—á–µ–Ω–∏–µ BERTopic –º–æ–¥–µ–ª–∏"""
        self.documents = documents
        
        # –æ–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        self.hdbscan_model = HDBSCAN(
            min_cluster_size=min_cluster_size,
            metric='euclidean',
            cluster_selection_method='eom',
            prediction_data=True
        )
        
        # —Å–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = BERTopic(
            embedding_model=self.sentence_model,
            umap_model=self.umap_model,
            hdbscan_model=self.hdbscan_model,
            vectorizer_model=self.vectorizer_model,
            calculate_probabilities=calculate_probabilities,
            verbose=True
        )
        
        # –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ
        if embeddings is not None:
            print("üìÇ –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
            self.embeddings = embeddings
        else:
            print("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
            self.embeddings = self.sentence_model.encode(documents, show_progress_bar=True)
        
        print("ü§ñ –û–±—É—á–µ–Ω–∏–µ BERTopic...")
        topics, probabilities = self.model.fit_transform(documents, self.embeddings)
        
        # –æ–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–º—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if nr_topics is not None and nr_topics != "auto":
            self.model.reduce_topics(documents, nr_topics=nr_topics)
        elif nr_topics == "auto":
            self.model.reduce_topics(documents, nr_topics="auto")
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.model.get_topic_info())} —Ç–µ–º")
        print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ outliers: {sum(1 for t in topics if t == -1)}")
    
    def get_topics(self, num_words: int = 10) -> List[List[Tuple[str, float]]]:
        """–ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã"""
        if self.model is None:
            raise ValueError("–º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
        
        topics = []
        topic_info = self.model.get_topic_info()
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–º—ã –ø–æ —Ä–∞–∑–º–µ—Ä—É (–∏—Å–∫–ª—é—á–∞—è outliers)
        valid_topics = topic_info[topic_info['Topic'] != -1].sort_values('Count', ascending=False)
        
        for _, row in valid_topics.iterrows():
            topic_id = row['Topic']
            try:
                topic_words = self.model.get_topic(topic_id)
                if topic_words:  # –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ç–µ–º–∞ –Ω–µ –ø—É—Å—Ç–∞—è
                    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
                    topic_words_limited = topic_words[:num_words]
                    topics.append(topic_words_limited)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ç–µ–º—ã {topic_id}: {e}")
                continue
        
        return topics
    
    def get_document_topics(self) -> List[int]:
        """–ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–º—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        if self.model is None:
            raise ValueError("–º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
        
        topics, _ = self.model.transform(self.documents, self.embeddings)
        return topics
    
    def get_topic_info(self) -> pd.DataFrame:
        """–ø–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–º–∞—Ö"""
        if self.model is None:
            raise ValueError("–º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
        
        return self.model.get_topic_info()
    
    def get_representative_docs(self, topic_id: int, num_docs: int = 5) -> List[str]:
        """–ø–æ–ª—É—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç–µ–º—ã"""
        if self.model is None:
            raise ValueError("–º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
        
        representative_docs = self.model.get_representative_docs(topic_id)
        return representative_docs[:num_docs]
    
    def save_model(self, filepath: str) -> None:
        """—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        if self.model is None:
            raise ValueError("–º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
        
        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        model_data = {
            'model': self.model,
            'embeddings': self.embeddings,
            'documents': self.documents,
            'embedding_model_name': self.embedding_model_name
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"–º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filepath}")
    
    def load_model(self, filepath: str) -> None:
        """–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.model = model_data['model']
        self.embeddings = model_data['embeddings']
        self.documents = model_data['documents']
        self.embedding_model_name = model_data['embedding_model_name']
        
        print(f"–º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {filepath}")


def tune_bertopic_clustering(documents: List[str],
                            min_cluster_sizes: List[int],
                            embedding_model: str = "paraphrase-multilingual-MiniLM-L12-v2",
                            random_state: int = 42) -> pd.DataFrame:
    """–ø–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ min_cluster_size –¥–ª—è BERTopic"""
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è coherence
    from src.data_preprocessing import TextPreprocessor
    preprocessor = TextPreprocessor()
    processed_texts = []
    for doc in documents:
        tokens = preprocessor.preprocess_text(doc)
        if tokens:  # —Ç–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            processed_texts.append(tokens)
    
    results = []
    
    for min_cluster_size in min_cluster_sizes:
        print(f"—Ç–µ—Å—Ç–∏—Ä—É–µ–º min_cluster_size={min_cluster_size}...")
        
        # —Å–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        bertopic = BERTopicModel(
            embedding_model=embedding_model,
            random_state=random_state
        )
        
        bertopic.train(
            documents=documents,
            min_cluster_size=min_cluster_size,
            calculate_probabilities=True
        )
        
        # —Å–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        topic_info = bertopic.get_topic_info()
        num_topics = len(topic_info) - 1 if -1 in topic_info['Topic'].values else len(topic_info)
        num_outliers = topic_info[topic_info['Topic'] == -1]['Count'].sum() if -1 in topic_info['Topic'].values else 0
        total_docs = len(documents)
        
        # —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —Ç–µ–º—ã
        if num_topics > 0:
            topic_sizes = topic_info[topic_info['Topic'] != -1]['Count']
            avg_topic_size = topic_sizes.mean()
            min_topic_size = topic_sizes.min()
            max_topic_size = topic_sizes.max()
        else:
            avg_topic_size = 0
            min_topic_size = 0
            max_topic_size = 0
        
        # —Ä–∞—Å—á–µ—Ç coherence_cv
        coherence_cv = 0.0
        try:
            topics = bertopic.get_topics()
            if topics and processed_texts:
                from src.evaluation import TopicEvaluator
                evaluator = TopicEvaluator()
                topic_words = []
                for topic in topics:
                    words = [word for word, _ in topic if isinstance(word, str)]
                    if words:
                        topic_words.append(words)
                
                if topic_words:
                    coherence_cv = evaluator.compute_coherence(topic_words, processed_texts, 'c_v')
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ coherence –¥–ª—è min_cluster_size={min_cluster_size}: {e}")
            coherence_cv = 0.0
        
        results.append({
            'min_cluster_size': min_cluster_size,
            'num_topics': num_topics,
            'outlier_ratio': num_outliers / total_docs,
            'coverage': (total_docs - num_outliers) / total_docs,
            'avg_topic_size': avg_topic_size,
            'min_topic_size': min_topic_size,
            'max_topic_size': max_topic_size,
            'coherence_cv': coherence_cv,
            'total_docs': total_docs,
            'outliers': num_outliers
        })
        
        print(f"  –Ω–∞–π–¥–µ–Ω–æ —Ç–µ–º: {num_topics}")
        print(f"  outliers: {num_outliers} ({num_outliers/total_docs:.1%})")
        print(f"  coherence_cv: {coherence_cv:.3f}")
    
    return pd.DataFrame(results)


def compute_topic_diversity(topics: List[List[Tuple[str, float]]], 
                           top_k: int = 10) -> float:
    """–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ç–µ–º (–¥–æ–ª—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤)"""
    all_words = set()
    topic_words = set()
    
    for topic in topics:
        words = [word for word, _ in topic[:top_k]]
        topic_words.update(words)
        all_words.update(words)
    
    if len(all_words) == 0:
        return 0.0
    
    return len(topic_words) / len(all_words) 