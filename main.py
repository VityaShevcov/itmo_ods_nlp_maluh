#!/usr/bin/env python3
"""
–û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
"""

import os
import sys
import argparse
import pickle
import pandas as pd
import numpy as np
from typing import List, Dict, Any

# –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –Ω–∞—à–∏–º –º–æ–¥—É–ª—è–º
sys.path.append('src')

from data_preprocessing import load_and_clean_data, TextPreprocessor
from lda_model import LDAModel, tune_lda_topics
from bertopic_model import BERTopicModel, tune_bertopic_clustering
from evaluation import TopicEvaluator, compare_models
from visualization import TopicVisualizer


def run_preprocessing(data_path: str, save_path: str = 'data/processed') -> tuple:
    """–∑–∞–ø—É—Å–∫ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    print("=== –≠–¢–ê–ü 1: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===")
    
    # —Å–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    os.makedirs(save_path, exist_ok=True)
    
    # –∑–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print("–∑–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    df = load_and_clean_data(data_path)
    print(f"–∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
    print("–ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...")
    preprocessor = TextPreprocessor()
    processed_texts = preprocessor.preprocess_corpus(df['text'].tolist())
    
    # —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    valid_indices = [i for i, tokens in enumerate(processed_texts) if len(tokens) > 0]
    df_filtered = df.iloc[valid_indices].copy()
    processed_texts_filtered = [processed_texts[i] for i in valid_indices]
    
    print(f"–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df_filtered)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    df_filtered.to_csv(f'{save_path}/processed_reviews.csv', index=False)
    with open(f'{save_path}/processed_texts.pkl', 'wb') as f:
        pickle.dump(processed_texts_filtered, f)
    
    print("–ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    return df_filtered, processed_texts_filtered


def run_lda_modeling(processed_texts: List[List[str]], 
                    save_path: str = 'models') -> Dict[str, Any]:
    """–∑–∞–ø—É—Å–∫ LDA –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("\n=== –≠–¢–ê–ü 2: LDA –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ ===")
    
    os.makedirs(save_path, exist_ok=True)
    
    # –ø–æ–¥–±–æ—Ä —á–∏—Å–ª–∞ —Ç–µ–º
    print("–ø–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ —Ç–µ–º...")
    topic_range = list(range(5, 31, 5))  # –æ—Ç 5 –¥–æ 30 —Å —à–∞–≥–æ–º 5
    tuning_results = tune_lda_topics(processed_texts, topic_range)
    
    # –Ω–∞—Ö–æ–¥–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–µ–º
    best_topics = tuning_results.loc[tuning_results['coherence_cv'].idxmax(), 'num_topics']
    print(f"–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–µ–º: {best_topics}")
    
    # –æ–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    print("–æ–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π LDA –º–æ–¥–µ–ª–∏...")
    lda_model = LDAModel(random_state=42)
    lda_model.prepare_corpus(processed_texts)
    lda_model.train(num_topics=int(best_topics))
    
    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    lda_model.save_model(f'{save_path}/lda_model.pkl')
    tuning_results.to_csv(f'{save_path}/lda_tuning_results.csv', index=False)
    
    return {
        'model': lda_model,
        'tuning_results': tuning_results,
        'best_topics': best_topics
    }


def run_bertopic_modeling(df: pd.DataFrame, 
                         save_path: str = 'models') -> Dict[str, Any]:
    """–∑–∞–ø—É—Å–∫ BERTopic –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("\n=== –≠–¢–ê–ü 3: BERTopic –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ ===")
    
    # –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è BERTopic
    documents = df['text'].tolist()
    
    # –ø–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    print("–ø–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...")
    cluster_sizes = [10, 15, 20, 30]
    tuning_results = tune_bertopic_clustering(documents, cluster_sizes)
    
    # –≤—ã–±–∏—Ä–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä (–±–∞–ª–∞–Ω—Å —á–∏—Å–ª–∞ —Ç–µ–º –∏ outliers)
    tuning_results['score'] = tuning_results['num_topics'] * (1 - tuning_results['outlier_ratio'])
    best_cluster_size = tuning_results.loc[tuning_results['score'].idxmax(), 'min_cluster_size']
    print(f"–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π min_cluster_size: {best_cluster_size}")
    
    # –æ–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    print("–æ–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π BERTopic –º–æ–¥–µ–ª–∏...")
    bertopic_model = BERTopicModel(random_state=42)
    bertopic_model.train(documents, min_cluster_size=int(best_cluster_size))
    
    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    bertopic_model.save_model(f'{save_path}/bertopic_model.pkl')
    tuning_results.to_csv(f'{save_path}/bertopic_tuning_results.csv', index=False)
    
    return {
        'model': bertopic_model,
        'tuning_results': tuning_results,
        'best_cluster_size': best_cluster_size
    }


def run_evaluation(lda_results: Dict[str, Any], 
                  bertopic_results: Dict[str, Any],
                  processed_texts: List[List[str]],
                  df: pd.DataFrame,
                  save_path: str = 'report') -> Dict[str, Any]:
    """–∑–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
    print("\n=== –≠–¢–ê–ü 4: –û—Ü–µ–Ω–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π ===")
    
    os.makedirs(save_path, exist_ok=True)
    os.makedirs(f'{save_path}/figures', exist_ok=True)
    
    evaluator = TopicEvaluator()
    
    # –ø–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–µ–π
    lda_model = lda_results['model']
    bertopic_model = bertopic_results['model']
    
    # —Ç–µ–º—ã –∏ –º–µ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    lda_topics = lda_model.get_topics()
    lda_doc_topics = lda_model.get_dominant_topics()
    
    bertopic_topics = bertopic_model.get_topics()
    bertopic_doc_topics = bertopic_model.get_document_topics()
    
    # —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è silhouette
    embeddings = bertopic_model.embeddings
    
    # –æ—Ü–µ–Ω–∫–∞ LDA
    print("–æ—Ü–µ–Ω–∫–∞ LDA –º–æ–¥–µ–ª–∏...")
    lda_eval = evaluator.evaluate_model(
        topics=lda_topics,
        texts=processed_texts,
        embeddings=embeddings,
        doc_topics=lda_doc_topics,
        model_name="LDA"
    )
    
    # –æ—Ü–µ–Ω–∫–∞ BERTopic
    print("–æ—Ü–µ–Ω–∫–∞ BERTopic –º–æ–¥–µ–ª–∏...")
    bertopic_eval = evaluator.evaluate_model(
        topics=bertopic_topics,
        texts=processed_texts,
        embeddings=embeddings,
        doc_topics=bertopic_doc_topics,
        model_name="BERTopic"
    )
    
    # —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    comparison_df = compare_models(lda_eval, bertopic_eval)
    
    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    comparison_df.to_csv(f'{save_path}/model_comparison.csv', index=False)
    
    with open(f'{save_path}/evaluation_results.pkl', 'wb') as f:
        pickle.dump({
            'lda_eval': lda_eval,
            'bertopic_eval': bertopic_eval,
            'comparison': comparison_df
        }, f)
    
    print("–æ—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    return {
        'lda_eval': lda_eval,
        'bertopic_eval': bertopic_eval,
        'comparison': comparison_df
    }


def create_visualizations(lda_results: Dict[str, Any], 
                         bertopic_results: Dict[str, Any],
                         evaluation_results: Dict[str, Any],
                         df: pd.DataFrame,
                         save_path: str = 'report/figures') -> None:
    """—Å–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π"""
    print("\n=== –≠–¢–ê–ü 5: –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π ===")
    
    viz = TopicVisualizer()
    
    # –≥—Ä–∞—Ñ–∏–∫–∏ –ø–æ–¥–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    print("–≥—Ä–∞—Ñ–∏–∫–∏ –ø–æ–¥–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
    viz.plot_coherence_scores(lda_results['tuning_results'])
    viz.plot_bertopic_tuning(bertopic_results['tuning_results'])
    
    # —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    print("—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–µ–π...")
    viz.plot_metrics_comparison(evaluation_results['comparison'])
    
    # —Ä–∞–∑–º–µ—Ä—ã —Ç–µ–º
    lda_doc_topics = lda_results['model'].get_dominant_topics()
    bertopic_doc_topics = bertopic_results['model'].get_document_topics()
    viz.plot_topic_sizes(lda_doc_topics, bertopic_doc_topics)
    
    # —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ —Ç–µ–º–∞–º
    viz.plot_source_distribution(df, lda_doc_topics, bertopic_doc_topics)
    
    print("–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω—ã")


def main():
    """–æ—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤')
    parser.add_argument('--data', default='data_for_analysis.csv', help='–ø—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º')
    parser.add_argument('--skip-preprocessing', action='store_true', help='–ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É')
    parser.add_argument('--skip-lda', action='store_true', help='–ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å LDA')
    parser.add_argument('--skip-bertopic', action='store_true', help='–ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å BERTopic')
    parser.add_argument('--skip-viz', action='store_true', help='–ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏')
    
    args = parser.parse_args()
    
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è")
    print("=" * 50)
    
    # –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    if not args.skip_preprocessing:
        df, processed_texts = run_preprocessing(args.data)
    else:
        df = pd.read_csv('data/processed/processed_reviews.csv')
        with open('data/processed/processed_texts.pkl', 'rb') as f:
            processed_texts = pickle.load(f)
    
    # LDA –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
    if not args.skip_lda:
        lda_results = run_lda_modeling(processed_texts)
    else:
        lda_model = LDAModel()
        lda_model.load_model('models/lda_model.pkl')
        lda_tuning = pd.read_csv('models/lda_tuning_results.csv')
        lda_results = {'model': lda_model, 'tuning_results': lda_tuning}
    
    # BERTopic –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
    if not args.skip_bertopic:
        bertopic_results = run_bertopic_modeling(df)
    else:
        bertopic_model = BERTopicModel()
        bertopic_model.load_model('models/bertopic_model.pkl')
        bertopic_tuning = pd.read_csv('models/bertopic_tuning_results.csv')
        bertopic_results = {'model': bertopic_model, 'tuning_results': bertopic_tuning}
    
    # –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π
    evaluation_results = run_evaluation(lda_results, bertopic_results, processed_texts, df)
    
    # –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    if not args.skip_viz:
        create_visualizations(lda_results, bertopic_results, evaluation_results, df)
    
    print("\n‚úÖ –ü–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–∞—Ö: models/, report/")
    print(f"üìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π: report/model_comparison.csv")


if __name__ == '__main__':
    main() 