#!/usr/bin/env python3
"""
–û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
–ò–î–ï–ê–õ–¨–ù–´–ô –ü–õ–ê–ù: –ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–µ—Ç—Ä–∏–∫ + –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–º
"""

import os
import sys
import argparse
import pickle
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional
import warnings

# –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –Ω–∞—à–∏–º –º–æ–¥—É–ª—è–º
sys.path.append('src')

from data_preprocessing import load_and_clean_data, TextPreprocessor
from lda_model import LDAModel, tune_lda_topics
from bertopic_model import BERTopicModel, tune_bertopic_clustering
from evaluation import TopicEvaluator, compare_models, find_optimal_topic_model
from visualization import TopicVisualizer, visualize_embeddings_comparison, visualize_metrics_comparison, visualize_comprehensive_comparison

warnings.filterwarnings("ignore", category=DeprecationWarning) 

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
DATA_PATH = "data/raw/data_for_analysis.csv"
EMBEDDINGS_CACHE = "models/embeddings_cache.pkl"
RESULTS_PATH = "results/"
FIGURES_PATH = os.path.join(RESULTS_PATH, "figures")
TABLES_PATH = os.path.join(RESULTS_PATH, "tables")

def ensure_dir(path):
    """–£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç."""
    os.makedirs(path, exist_ok=True)

def load_or_create_embeddings(documents: List[str], 
                             embeddings_path: str = 'models/embeddings.pkl') -> np.ndarray:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    if os.path.exists(embeddings_path):
        print("üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
        with open(embeddings_path, 'rb') as f:
            embeddings = pickle.load(f)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {embeddings.shape[0]} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ {embeddings.shape[1]}")
        return embeddings
    else:
        print("üîÑ –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
        from sentence_transformers import SentenceTransformer
        
        # –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        embeddings = model.encode(documents, show_progress_bar=True)
        
        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        os.makedirs(os.path.dirname(embeddings_path), exist_ok=True)
        with open(embeddings_path, 'wb') as f:
            pickle.dump(embeddings, f)
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {embeddings.shape[0]} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
        return embeddings


def evaluate_with_composite_metrics(topics: List[List[str]], 
                                   texts: List[List[str]], 
                                   embeddings: np.ndarray,
                                   doc_topics: List[int],
                                   evaluator: TopicEvaluator) -> Dict[str, float]:
    """
    –ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ —Å –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö (Egger & Yu 2022, Meaney et al. 2023)
    """
    # –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    coherence_cv = evaluator.compute_coherence(topics, texts, 'c_v')
    coherence_umass = evaluator.compute_coherence(topics, texts, 'u_mass') 
    silhouette = evaluator.compute_silhouette(embeddings, doc_topics)
    coverage_info = evaluator.compute_topic_coverage(doc_topics)
    coverage = coverage_info['coverage']
    
    # –∫–æ–º–ø–æ–∑–∏—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (–æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ Meaney et al. 2023)
    # –í–µ—Å–∞ –ø–æ–¥–æ–±—Ä–∞–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫ –≤ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–µ
    composite_score = (
        0.35 * coherence_cv +      # —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å (–æ—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)
        0.25 * silhouette +        # –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏  
        0.25 * coverage +          # –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        0.15 * (-coherence_umass)  # —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–µ–º (u_mass –æ–±—ã—á–Ω–æ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è)
    )
    
    return {
        'coherence_cv': coherence_cv,
        'coherence_umass': coherence_umass,
        'silhouette': silhouette,
        'coverage': coverage,
        'composite_score': composite_score
    }


def run_preprocessing(data_path: str, save_path: str = 'data/processed') -> tuple:
    """–∑–∞–ø—É—Å–∫ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    print("=== –≠–¢–ê–ü 1: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===")
    
    # —Å–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    os.makedirs(save_path, exist_ok=True)
    
    # –∑–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print("–∑–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    df = load_and_clean_data(data_path)
    print(f"–∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
    print("–ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...")
    preprocessor = TextPreprocessor()
    processed_texts = preprocessor.preprocess_corpus(df['text'].tolist())
    
    # —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    valid_indices = [i for i, tokens in enumerate(processed_texts) if len(tokens) > 0]
    df_filtered = df.iloc[valid_indices].copy()
    processed_texts_filtered = [processed_texts[i] for i in valid_indices]
    
    print(f"–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df_filtered)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    df_filtered.to_csv(f'{save_path}/processed_reviews.csv', index=False)
    with open(f'{save_path}/processed_texts.pkl', 'wb') as f:
        pickle.dump(processed_texts_filtered, f)
    
    print("–ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    return df_filtered, processed_texts_filtered


def run_lda_modeling_with_metrics(processed_texts: List[List[str]], 
                                 embeddings: np.ndarray,
                                 save_path: str = 'models') -> Dict[str, Any]:
    """LDA –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –µ–¥–∏–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π –º–µ—Ç—Ä–∏–∫"""
    print("\n=== –≠–¢–ê–ü 2: LDA –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (–ø–æ –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º) ===")
    
    os.makedirs(save_path, exist_ok=True)
    evaluator = TopicEvaluator()
    
    # –¥–∏–∞–ø–∞–∑–æ–Ω —Ç–µ–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    topic_range = list(range(10, 36, 5))  # [10, 15, 20, 25, 30, 35]
    print(f"—Ç–µ—Å—Ç–∏—Ä—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º: {topic_range}")
    
    tuning_results = []
    best_score = -float('inf')
    best_model = None
    best_topics = 0
    
    for num_topics in topic_range:
        print(f"\nüîÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º {num_topics} —Ç–µ–º...")
        
        # –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        lda_model = LDAModel(random_state=42)
        lda_model.prepare_corpus(processed_texts)
        lda_model.train(num_topics=num_topics)
        
        # –ø–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        topics = lda_model.get_topics()
        doc_topics = lda_model.get_dominant_topics()
        
        # –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–º—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        topic_words = [[word for word, _ in topic] for topic in topics]
        
        # –æ—Ü–µ–Ω–∏–≤–∞–µ–º –ø–æ –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º
        metrics = evaluate_with_composite_metrics(
            topic_words, processed_texts, embeddings, doc_topics, evaluator
        )
        
        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            'num_topics': num_topics,
            **metrics
        }
        tuning_results.append(result)
        
        print(f"  üìä –ö–æ–º–ø–æ–∑–∏—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {metrics['composite_score']:.3f}")
        print(f"     - Coherence CV: {metrics['coherence_cv']:.3f}")
        print(f"     - Silhouette: {metrics['silhouette']:.3f}")
        print(f"     - Coverage: {metrics['coverage']:.3f}")
        
        # –æ–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if metrics['composite_score'] > best_score:
            best_score = metrics['composite_score']
            best_model = lda_model
            best_topics = num_topics
    
    print(f"\nüèÜ –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_topics} —Ç–µ–º (–∫–æ–º–ø–æ–∑–∏—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {best_score:.3f})")
    
    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    best_model.save_model(f'{save_path}/lda_model_optimal.pkl')
    tuning_df = pd.DataFrame(tuning_results)
    tuning_df.to_csv(f'{save_path}/lda_tuning_metrics.csv', index=False)
    
    return {
        'model': best_model,
        'tuning_results': tuning_df,
        'best_topics': best_topics,
        'best_score': best_score
    }


def run_bertopic_modeling_with_metrics(df: pd.DataFrame, 
                                      embeddings: np.ndarray,
                                      save_path: str = 'models') -> Dict[str, Any]:
    """BERTopic –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –µ–¥–∏–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π –º–µ—Ç—Ä–∏–∫"""
    print("\n=== –≠–¢–ê–ü 3: BERTopic –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (–ø–æ –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º) ===")
    
    documents = df['text'].tolist()
    evaluator = TopicEvaluator()
    
    # –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    cluster_sizes = [5, 10, 15, 20, 25]
    print(f"—Ç–µ—Å—Ç–∏—Ä—É–µ–º min_cluster_size: {cluster_sizes}")
    
    tuning_results = []
    best_score = -float('inf')
    best_model = None
    best_cluster_size = 0
    
    for cluster_size in cluster_sizes:
        print(f"\nüîÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º min_cluster_size={cluster_size}...")
        
        # –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        bertopic_model = BERTopicModel(random_state=42)
        bertopic_model.train(
            documents, 
            min_cluster_size=cluster_size,
            nr_topics="auto",
            calculate_probabilities=True,
            embeddings=embeddings  # –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        )
        
        # –ø–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        topics = bertopic_model.get_topics()
        doc_topics = bertopic_model.get_document_topics()
        
        # –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–º—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (—É–±–∏—Ä–∞–µ–º –≤–µ—Å–∞ –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º —Ñ—Ä–∞–∑—ã)
        topic_words = []
        for topic in topics:
            words = []
            for word, _ in topic:
                if ' ' in word:  # —Ñ—Ä–∞–∑–∞ - —Ä–∞–∑–±–∏–≤–∞–µ–º
                    words.extend(word.split())
                else:
                    words.append(word)
            topic_words.append(list(dict.fromkeys(words)))  # —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        
        # –ø–æ–ª—É—á–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è coherence
        preprocessor = TextPreprocessor()
        processed_texts = preprocessor.preprocess_corpus(documents)
        
        # –æ—Ü–µ–Ω–∏–≤–∞–µ–º –ø–æ –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º
        metrics = evaluate_with_composite_metrics(
            topic_words, processed_texts, embeddings, doc_topics, evaluator
        )
        
        # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        num_topics = len([t for t in set(doc_topics) if t != -1])
        coverage_info = evaluator.compute_topic_coverage(doc_topics)
        
        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            'min_cluster_size': cluster_size,
            'num_topics': num_topics,
            'outlier_ratio': coverage_info['outlier_ratio'],
            **metrics
        }
        tuning_results.append(result)
        
        print(f"  üìä –ö–æ–º–ø–æ–∑–∏—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {metrics['composite_score']:.3f}")
        print(f"     - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º: {num_topics}")
        print(f"     - Coherence CV: {metrics['coherence_cv']:.3f}")
        print(f"     - Silhouette: {metrics['silhouette']:.3f}")
        print(f"     - Coverage: {metrics['coverage']:.3f}")
        
        # –æ–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if metrics['composite_score'] > best_score:
            best_score = metrics['composite_score']
            best_model = bertopic_model
            best_cluster_size = cluster_size
    
    print(f"\nüèÜ –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: min_cluster_size={best_cluster_size} (–∫–æ–º–ø–æ–∑–∏—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {best_score:.3f})")
    
    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    best_model.save_model(f'{save_path}/bertopic_model_optimal.pkl')
    tuning_df = pd.DataFrame(tuning_results)
    tuning_df.to_csv(f'{save_path}/bertopic_tuning_metrics.csv', index=False)
    
    return {
        'model': best_model,
        'tuning_results': tuning_df,
        'best_cluster_size': best_cluster_size,
        'best_score': best_score
    }


def run_additional_lda_comparison(processed_texts: List[List[str]], 
                                 bertopic_num_topics: int,
                                 embeddings: np.ndarray,
                                 save_path: str = 'models') -> Dict[str, Any]:
    """
    –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û–ï –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï: LDA —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–º –∫–∞–∫ —É BERTopic
    –î–ª—è —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ç–µ–º
    """
    print(f"\n=== –≠–¢–ê–ü 4: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ LDA —Å {bertopic_num_topics} —Ç–µ–º–∞–º–∏ ===")
    print("üîç –¶–µ–ª—å: —Å—Ä–∞–≤–Ω–∏—Ç—å –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ç–µ–º")
    
    evaluator = TopicEvaluator()
    
    # –æ–±—É—á–∞–µ–º LDA —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–º
    print(f"üîÑ –û–±—É—á–∞–µ–º LDA —Å {bertopic_num_topics} —Ç–µ–º–∞–º–∏...")
    lda_fixed = LDAModel(random_state=42)
    lda_fixed.prepare_corpus(processed_texts)
    lda_fixed.train(num_topics=bertopic_num_topics)
    
    # –ø–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    topics = lda_fixed.get_topics()
    doc_topics = lda_fixed.get_dominant_topics()
    topic_words = [[word for word, _ in topic] for topic in topics]
    
    # –æ—Ü–µ–Ω–∏–≤–∞–µ–º –ø–æ –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º
    metrics = evaluate_with_composite_metrics(
        topic_words, processed_texts, embeddings, doc_topics, evaluator
    )
    
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã LDA —Å {bertopic_num_topics} —Ç–µ–º–∞–º–∏:")
    print(f"   - –ö–æ–º–ø–æ–∑–∏—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {metrics['composite_score']:.3f}")
    print(f"   - Coherence CV: {metrics['coherence_cv']:.3f}")
    print(f"   - Silhouette: {metrics['silhouette']:.3f}")
    print(f"   - Coverage: {metrics['coverage']:.3f}")
    
    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    lda_fixed.save_model(f'{save_path}/lda_model_fixed_{bertopic_num_topics}.pkl')
    
    return {
        'model': lda_fixed,
        'num_topics': bertopic_num_topics,
        'metrics': metrics
    }


def run_comprehensive_evaluation(lda_optimal: Dict[str, Any],
                               bertopic_optimal: Dict[str, Any], 
                               lda_fixed: Dict[str, Any],
                               processed_texts: List[List[str]],
                               df: pd.DataFrame,
                               embeddings: np.ndarray,
                               save_path: str = 'report') -> Dict[str, Any]:
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
    print("\n=== –≠–¢–ê–ü 5: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ ===")
    
    os.makedirs(save_path, exist_ok=True)
    evaluator = TopicEvaluator()
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    models_comparison = []
    
    # 1. LDA –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è (–ø–æ –º–µ—Ç—Ä–∏–∫–∞–º)
    lda_opt_topics = [[word for word, _ in topic] for topic in lda_optimal['model'].get_topics()]
    lda_opt_doc_topics = lda_optimal['model'].get_dominant_topics()
    lda_opt_metrics = evaluate_with_composite_metrics(
        lda_opt_topics, processed_texts, embeddings, lda_opt_doc_topics, evaluator
    )
    
    models_comparison.append({
        'model': 'LDA_Optimal',
        'num_topics': lda_optimal['best_topics'],
        'selection_method': '–ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏',
        **lda_opt_metrics
    })
    
    # 2. BERTopic –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è (–ø–æ –º–µ—Ç—Ä–∏–∫–∞–º)
    bertopic_topics = bertopic_optimal['model'].get_topics()
    bertopic_doc_topics = bertopic_optimal['model'].get_document_topics()
    
    # –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º BERTopic —Ç–µ–º—ã
    bertopic_topic_words = []
    for topic in bertopic_topics:
        words = []
        for word, _ in topic:
            if ' ' in word:
                words.extend(word.split())
            else:
                words.append(word)
        bertopic_topic_words.append(list(dict.fromkeys(words)))
    
    bertopic_metrics = evaluate_with_composite_metrics(
        bertopic_topic_words, processed_texts, embeddings, bertopic_doc_topics, evaluator
    )
    
    bertopic_num_topics = len([t for t in set(bertopic_doc_topics) if t != -1])
    models_comparison.append({
        'model': 'BERTopic_Optimal',
        'num_topics': bertopic_num_topics,
        'selection_method': '–ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏',
        **bertopic_metrics
    })
    
    # 3. LDA —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–º (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ)
    models_comparison.append({
        'model': f'LDA_Fixed_{lda_fixed["num_topics"]}',
        'num_topics': lda_fixed['num_topics'],
        'selection_method': '–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ (–∫–∞–∫ BERTopic)',
        **lda_fixed['metrics']
    })
    
    # —Å–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    comparison_df = pd.DataFrame(models_comparison)
    
    # –¥–æ–±–∞–≤–ª—è–µ–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –∫–æ–º–ø–æ–∑–∏—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–µ
    comparison_df['rank_composite'] = comparison_df['composite_score'].rank(ascending=False)
    comparison_df['rank_coherence'] = comparison_df['coherence_cv'].rank(ascending=False)
    comparison_df['rank_silhouette'] = comparison_df['silhouette'].rank(ascending=False)
    comparison_df['rank_coverage'] = comparison_df['coverage'].rank(ascending=False)
    
    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    comparison_df.to_csv(f'{save_path}/comprehensive_model_comparison.csv', index=False)
    
    # –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    detailed_results = {
        'lda_optimal': lda_optimal,
        'bertopic_optimal': bertopic_optimal,
        'lda_fixed': lda_fixed,
        'comparison': comparison_df,
        'academic_justification': {
            'composite_metrics_source': 'Meaney et al. (2023), Egger & Yu (2022)',
            'weights_rationale': {
                'coherence_cv': 0.35,  # –æ—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏
                'silhouette': 0.25,    # –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
                'coverage': 0.25,      # –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                'coherence_umass': 0.15  # —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–µ–º
            },
            'fixed_topics_rationale': '–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –≤–ª–∏—è–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–µ–º –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã'
        }
    }
    
    with open(f'{save_path}/detailed_evaluation_results.pkl', 'wb') as f:
        pickle.dump(detailed_results, f)
    
    print("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:")
    print(comparison_df[['model', 'num_topics', 'composite_score', 'coherence_cv', 'silhouette', 'coverage']].round(3))
    
    return detailed_results


def create_enhanced_visualizations(results: Dict[str, Any],
                                 df: pd.DataFrame,
                                 processed_texts: List[List[str]],
                                 embeddings: np.ndarray,
                                 save_path: str = 'report/figures') -> None:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π"""
    print("\n=== –≠–¢–ê–ü 6: –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π ===")
    
    os.makedirs(save_path, exist_ok=True)
    viz = TopicVisualizer()
    
    # 1. –ì—Ä–∞—Ñ–∏–∫–∏ –ø–æ–¥–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    print("üìà –ì—Ä–∞—Ñ–∏–∫–∏ –ø–æ–¥–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
    lda_tuning = results['lda_optimal']['tuning_results']
    bertopic_tuning = results['bertopic_optimal']['tuning_results']
    
    viz.plot_composite_metrics_tuning(lda_tuning, 'LDA')
    viz.plot_composite_metrics_tuning(bertopic_tuning, 'BERTopic')
    
    # 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    print("üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
    comparison_df = results['comparison']
    viz.plot_comprehensive_comparison(comparison_df)
    
    # 3. –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–º
    print("üîç –ê–Ω–∞–ª–∏–∑ —Ç–µ–º...")
    lda_opt_model = results['lda_optimal']['model']
    bertopic_opt_model = results['bertopic_optimal']['model']
    lda_fixed_model = results['lda_fixed']['model']
    
    # —Ä–∞–∑–º–µ—Ä—ã —Ç–µ–º –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    lda_opt_topics = lda_opt_model.get_dominant_topics()
    bertopic_topics = bertopic_opt_model.get_document_topics()
    lda_fixed_topics = lda_fixed_model.get_dominant_topics()
    
    viz.plot_topic_sizes_comparison(lda_opt_topics, bertopic_topics, lda_fixed_topics)
    
    # 4. UMAP –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    print("üó∫Ô∏è  UMAP –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è...")
    viz.plot_embeddings_comparison(embeddings, lda_opt_topics, bertopic_topics, lda_fixed_topics)
    
    # 5. –ê–Ω–∞–ª–∏–∑ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–≥–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è
    print("üìö –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º...")
    academic_info = results['academic_justification']
    viz.create_metrics_justification_report(academic_info, save_path)
    
    print("‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω—ã")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –ò–î–ï–ê–õ–¨–ù–´–ô –ü–õ–ê–ù"""
    parser = argparse.ArgumentParser(description='–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ: –ò–î–ï–ê–õ–¨–ù–´–ô –ü–õ–ê–ù')
    parser.add_argument('--data', default='data_for_analysis.csv', help='–ø—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º')
    parser.add_argument('--skip-preprocessing', action='store_true', help='–ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É')
    parser.add_argument('--skip-embeddings', action='store_true', help='–ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤')
    parser.add_argument('--skip-viz', action='store_true', help='–ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏')
    
    args = parser.parse_args()
    
    print("üöÄ –ó–ê–ü–£–°–ö –ü–ê–ô–ü–õ–ê–ô–ù–ê: –ò–î–ï–ê–õ–¨–ù–´–ô –ü–õ–ê–ù")
    print("=" * 60)
    print("üìã –ü–ª–∞–Ω:")
    print("   1. –ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è LDA –∏ BERTopic")
    print("   2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è")
    print("   3. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ LDA —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–º")
    print("   4. –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫")
    print("=" * 60)
    
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Å–µ –ø–∞–ø–∫–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
    ensure_dir(RESULTS_PATH)
    ensure_dir(FIGURES_PATH)
    ensure_dir(TABLES_PATH)

    # --- –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---
    print("\n--- –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---")
    if os.path.exists(EMBEDDINGS_CACHE):
        print(f"–ö—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–∞–π–¥–µ–Ω. –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ {EMBEDDINGS_CACHE}...")
        with open(EMBEDDINGS_CACHE, 'rb') as f:
            embeddings = pickle.load(f)
    else:
        print(f"–ö—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {DATA_PATH}...")
        df = pd.read_csv(DATA_PATH)
        df_filtered = df[df['text'].str.len() > 50].copy()
        
        print("–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞...")
        preprocessor = TextPreprocessor()
        processed_texts = preprocessor.preprocess_corpus(df_filtered['text'].tolist())
        
        print("–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        embeddings = load_or_create_embeddings(df_filtered['text'].tolist(), EMBEDDINGS_CACHE)

    # --- –®–∞–≥ 2: –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π ---
    print("\n--- –®–∞–≥ 2: –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π ---")
    lda_results = find_optimal_topic_model(LDAModel, processed_texts, embeddings, range(10, 36, 5))
    bertopic_results = find_optimal_topic_model(BERTopicModel, df_filtered, embeddings, [5, 10, 15, 20, 25])
    
    optimal_lda_params = lda_results['best_params']
    best_lda_score = lda_results['best_score']
    optimal_bertopic_params = bertopic_results['best_params']
    best_bertopic_score = bertopic_results['best_score']
    
    print(f"–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å LDA: {optimal_lda_params} —Å –∫–æ–º–ø–æ–∑–∏—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π {best_lda_score:.3f}")
    print(f"–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å BERTopic: {optimal_bertopic_params} —Å –∫–æ–º–ø–æ–∑–∏—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π {best_bertopic_score:.3f}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ pkl
    with open(os.path.join(RESULTS_PATH, 'detailed_evaluation_results.pkl'), 'wb') as f:
        pickle.dump({
            'lda_results': lda_results,
            'bertopic_results': bertopic_results
        }, f)

    # --- –®–∞–≥ 3: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ ---
    print("\n--- –®–∞–≥ 3: –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ ---")
    lda_model = LDAModel(random_state=42)
    lda_model.prepare_corpus(processed_texts)
    lda_model.train(num_topics=optimal_lda_params['num_topics'])
    
    bertopic_model = BERTopicModel(random_state=42)
    bertopic_model.train(
        df_filtered['text'].tolist(),
        min_cluster_size=optimal_bertopic_params['min_cluster_size'],
        nr_topics="auto",
        calculate_probabilities=True,
        embeddings=embeddings
    )
    
    # --- –®–∞–≥ 4: –°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–º ---
    print("\n--- –®–∞–≥ 4: –°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ ---")
    fair_lda_model_params = {'num_topics': optimal_bertopic_params['n_topics']}
    fair_lda_model = LDAModel(random_state=42)
    fair_lda_model.prepare_corpus(processed_texts)
    fair_lda_model.train(num_topics=fair_lda_model_params['num_topics'])
    
    # --- –®–∞–≥ 5: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ---
    print("\n--- –®–∞–≥ 5: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ---")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
    comparison_df = pd.DataFrame([
        {
            'Model': 'LDA',
            'Topics': optimal_lda_params['num_topics'],
            'Composite Score': best_lda_score,
            'Coherence CV': lda_results['best_metrics']['coherence_cv'],
            'Silhouette': lda_results['best_metrics']['silhouette'],
            'Coverage': lda_results['best_metrics']['coverage']
        },
        {
            'Model': 'BERTopic',
            'Topics': optimal_bertopic_params['n_topics'],
            'Composite Score': best_bertopic_score,
            'Coherence CV': bertopic_results['best_metrics']['coherence_cv'],
            'Silhouette': bertopic_results['best_metrics']['silhouette'],
            'Coverage': bertopic_results['best_metrics']['coverage']
        },
        {
            'Model': 'Fair LDA',
            'Topics': fair_lda_model_params['num_topics'],
            'Composite Score': fair_lda_score,
            'Coherence CV': fair_lda_metrics['coherence_cv'],
            'Silhouette': fair_lda_metrics['silhouette'],
            'Coverage': fair_lda_metrics['coverage']
        }
    ])
    comparison_df.to_csv(os.path.join(TABLES_PATH, 'model_comparison.csv'), index=False)
    
    print("–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞:")
    print(comparison_df.to_markdown(index=False))
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    visualize_embeddings_comparison(lda_model, bertopic_model, embeddings, df_filtered, os.path.join(FIGURES_PATH, 'embeddings_comparison.png'))
    visualize_metrics_comparison(lda_results, bertopic_results, os.path.join(FIGURES_PATH, 'metrics_comparison.png'))
    visualize_comprehensive_comparison(comparison_df, os.path.join(FIGURES_PATH, 'comprehensive_comparison.png'))
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–µ–º –¥–ª—è –æ—Ç—á–µ—Ç–∞
    lda_topics = lda_model.show_topics(num_topics=optimal_lda_params['num_topics'], num_words=10, formatted=False)
    bertopic_topics = bertopic_model.get_topic_info()
    
    topic_examples = {
        "LDA": {f"Topic {t[0]}": [w[0] for w in t[1]] for t in lda_topics},
        "BERTopic": {f"Topic {row.Topic}": row.Name for _, row in bertopic_topics.iterrows() if row.Topic != -1}
    }
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    lda_topic_df = pd.DataFrame([f"–¢–µ–º–∞ {t[0]}: {', '.join([w[0] for w in t[1][:7]])}" for t in lda_topics], columns=['LDA Topics'])
    bertopic_topic_df = pd.DataFrame([f"–¢–µ–º–∞ {row.Topic}: {row.Name}" for _, row in bertopic_topics.iterrows() if row.Topic != -1], columns=['BERTopic Topics'])
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä—ã —Ç–µ–º –≤ CSV
    pd.concat([lda_topic_df, bertopic_topic_df], axis=1).to_csv(os.path.join(TABLES_PATH, 'topic_examples.csv'), index=False)


    print("\n–ò–î–ï–ê–õ–¨–ù–´–ô –ü–õ–ê–ù: –ó–ê–í–ï–†–®–ï–ù")
    print("="*30)


if __name__ == '__main__':
    main() 