# Сравнительный анализ методов тематического моделирования: LDA vs BERTopic на корпусе русскоязычных отзывов

## Аннотация

В данной работе представлен сравнительный анализ двух подходов к тематическому моделированию: классического вероятностного метода Latent Dirichlet Allocation (LDA) и современного подхода на основе эмбеддингов BERTopic. Исследование проведено на корпусе русскоязычных отзывов с платформ 2GIS и Banki.ru объемом 5,858 документов. Результаты показывают превосходство LDA в когерентности тем и покрытии данных, в то время как BERTopic демонстрирует более детальную сегментацию при значительной доле неклассифицированных документов.

**Ключевые слова:** тематическое моделирование, LDA, BERTopic, анализ текста, русский язык, машинное обучение

## 1. Введение

Тематическое моделирование является одной из ключевых задач в области обработки естественного языка и анализа текстов. С развитием технологий машинного обучения появились различные подходы к решению этой задачи, от классических вероятностных методов до современных подходов на основе трансформеров.

### 1.1 Цель исследования

Целью данного исследования является сравнительный анализ эффективности классического метода LDA и современного подхода BERTopic для тематического моделирования русскоязычных текстов в домене отзывов пользователей.

### 1.2 Задачи исследования

1. Провести предварительную обработку корпуса русскоязычных отзывов
2. Реализовать и оптимизировать параметры моделей LDA и BERTopic
3. Провести сравнительную оценку качества моделей по множественным метрикам
4. Проанализировать полученные результаты и сформулировать рекомендации

## 2. Обзор литературы и методов

### 2.1 Latent Dirichlet Allocation (LDA)

LDA представляет собой генеративную вероятностную модель, предложенную Blei et al. (2003). Модель основана на предположении, что документы представляют собой смеси тем, а темы — смеси слов. LDA использует распределения Дирихле в качестве априорных распределений для тем в документах и слов в темах.

### 2.2 BERTopic

BERTopic (Grootendorst, 2022) представляет современный подход к тематическому моделированию, основанный на трансформер-архитектурах. Метод включает следующие этапы:
1. Создание эмбеддингов документов с помощью предобученных языковых моделей
2. Снижение размерности с помощью UMAP
3. Кластеризация с использованием HDBSCAN
4. Создание представлений тем через c-TF-IDF

## 3. Методология

### 3.1 Данные

Для исследования использован корпус русскоязычных отзывов, собранный с платформ 2GIS и Banki.ru. Характеристики датасета:

- **Общее количество документов:** 5,858
- **После фильтрации:** 5,646 документов
- **Источники:** 2GIS, Banki.ru
- **Язык:** русский

### 3.2 Предобработка данных

Предобработка включала следующие этапы:

1. **Очистка текста:**
   - Удаление HTML-тегов и URL
   - Нормализация пробелов
   - Фильтрация по длине (минимум 10 символов)

2. **Токенизация и лемматизация:**
   - Токенизация с помощью NLTK
   - Лемматизация с использованием pymorphy2
   - Удаление стоп-слов

3. **Фильтрация словаря:**
   - Исключение слов с частотой < 5 документов
   - Исключение слов с частотой > 50% документов

### 3.3 Настройка моделей

#### 3.3.1 LDA
- **Библиотека:** Gensim LdaMulticore
- **Диапазон тем:** 5-30
- **Критерий оптимизации:** Coherence C_V
- **Параметры:** alpha='symmetric', eta='auto', passes=10

#### 3.3.2 BERTopic
- **Модель эмбеддингов:** paraphrase-multilingual-MiniLM-L12-v2
- **Снижение размерности:** UMAP (n_components=5, n_neighbors=15)
- **Кластеризация:** HDBSCAN (min_cluster_size: 10, 15, 20, 30)
- **Критерий оптимизации:** Соотношение количества тем и outliers

### 3.4 Метрики оценки

1. **Coherence C_V** - семантическая согласованность тем
2. **Coherence U_Mass** - статистическая согласованность
3. **Silhouette Score** - качество кластеризации в пространстве эмбеддингов
4. **Topic Diversity** - разнообразие словарей тем
5. **Coverage** - доля документов, получивших тематическую метку
6. **Perplexity** - перплексия модели (только для LDA)

## 4. Результаты

### 4.1 Оптимизация параметров

#### 4.1.1 LDA
Результаты подбора количества тем:

| Количество тем | Coherence C_V | Coherence U_Mass | Perplexity |
|---------------|---------------|------------------|------------|
| 5             | 0.414         | -1.755           | 1,284.57   |
| 10            | 0.409         | -1.899           | 1,809.70   |
| 15            | 0.389         | -2.142           | 2,366.82   |
| 20            | 0.380         | -2.189           | 3,102.36   |
| 25            | 0.367         | -2.193           | 3,962.31   |
| 30            | 0.379         | -2.387           | 5,166.54   |

**Оптимальное количество тем:** 5 (максимальная coherence C_V)

#### 4.1.2 BERTopic
Результаты подбора min_cluster_size:

| min_cluster_size | Количество тем | Outliers | Доля outliers | Средний размер темы |
|------------------|----------------|----------|---------------|---------------------|
| 10               | 68             | 2,516    | 44.6%         | 46.0                |
| 15               | 39             | 2,313    | 41.0%         | 85.5                |
| 20               | 36             | 2,483    | 44.0%         | 87.9                |
| 30               | 2              | 10       | 0.2%          | 2,818.0             |

**Оптимальный min_cluster_size:** 10 (лучший баланс количества тем и outliers)

### 4.2 Сравнительная оценка моделей

| Метрика         | LDA    | BERTopic | Разность |
|-----------------|--------|----------|----------|
| Coherence C_V   | 0.541  | 0.000    | -0.541   |
| Coherence U_Mass| -1.545 | 0.000    | +1.545   |
| Silhouette      | -0.015 | +0.016   | +0.031   |
| Topic Diversity | 0.740  | 0.675    | -0.065   |
| Coverage        | 100%   | 55.4%    | -44.6%   |
| Количество тем  | 5      | 68       | +63      |
| Средний размер  | 1,129  | 46       | -1,083   |

### 4.3 Качественный анализ

#### 4.3.1 LDA Topics (детальный анализ)

**Тема 0 - Банковские документы и сделки (22.5% документов)**
Ключевые слова: банк, сбербанк, документ, ипотека, кредит, сделка, сотрудник, менеджер, договор
*Интерпретация: Оформление кредитов, ипотеки и других банковских продуктов*

**Тема 1 - Банковские карты и обслуживание (22.5% документов)**
Ключевые слова: карта, банк, сбер, сбербанк, сотрудник, деньга, вопрос
*Интерпретация: Проблемы и вопросы по банковским картам*

**Тема 2 - Счета и денежные операции (22.5% документов)**
Ключевые слова: банк, деньга, счёт, сбербанк, обращение, средство, карта, сумма
*Интерпретация: Операции со счетами, переводы, управление средствами*

**Тема 3 - Платежи и цифровые услуги (22.5% документов)**
Ключевые слова: платёж, домклик, услуга, ипотека, ставка, оплата, страхование
*Интерпретация: Онлайн-платежи, страхование, цифровые сервисы*

**Тема 4 - Качество обслуживания в отделениях (22.5% документов)**
Ключевые слова: очередь, сотрудник, работать, отделение, банк, час, минута, окно, талон
*Интерпретация: Время ожидания, работа персонала, организация работы отделений*

#### 4.3.2 BERTopic Topics (топ-10 по размеру)

1. **Тема 0 - Благодарности (235 документов, 4.2%)**
   Ключевые слова: благодарность, спасибо, профессионализм, быстро
   
2. **Тема 1 - Мобильное приложение (208 документов, 3.7%)**
   Ключевые слова: приложение, установить, телефон, айфон
   
3. **Тема 2 - Ипотека и недвижимость (171 документ, 3.0%)**
   Ключевые слова: дом, ипотека, строительство, недвижимость
   
4. **Тема 3 - Общие вопросы банка (145 документов, 2.6%)**
   Ключевые слова: банк, деньги, сбербанк, банкоматы
   
5. **Тема 4 - Время работы (141 документ, 2.5%)**
   Ключевые слова: работы, суббота, график, закрыто
   
6. **Тема 5 - Удобство приложения (109 документов, 1.9%)**
   Ключевые слова: удобное приложение, пользуюсь, нравится
   
7. **Тема 6 - География услуг (104 документа, 1.8%)**
   Ключевые слова: Россия, РФ, Москва, Москве
   
8. **Тема 7 - Страхование (89 документов, 1.6%)**
   Ключевые слова: страховка, страхование, домклик
   
9. **Тема 8 - Банкоматы (84 документа, 1.5%)**
   Ключевые слова: банкоматы, банкомат, закрыт
   
10. **Тема 9 - Очереди в отделениях (80 документов, 1.4%)**
    Ключевые слова: очереди, медленно, сотрудники, отделение

## 5. Обсуждение

### 5.1 Преимущества LDA
1. **Высокая когерентность тем** (0.541 vs 0.000)
2. **Полное покрытие данных** (100% vs 55.4%)
3. **Интерпретируемость** - выделение широких тематических категорий
4. **Стабильность** - меньшая чувствительность к параметрам

### 5.2 Преимущества BERTopic
1. **Детальная сегментация** - выделение узкоспециализированных тем
2. **Лучшее качество кластеризации** (Silhouette: +0.016 vs -0.015)
3. **Использование семантических эмбеддингов** - учет контекста

### 5.3 Ограничения исследования
1. Высокая доля outliers в BERTopic (44.6%)
2. Различные принципы работы методов затрудняют прямое сравнение
3. Отсутствие coherence метрик для BERTopic из-за архитектурных особенностей

## 6. Заключение

Исследование показало, что выбор метода тематического моделирования зависит от конкретных задач:

### 6.1 Рекомендации по применению

**LDA рекомендуется для:**
- Получения общего представления о тематической структуре корпуса
- Задач, требующих полного покрытия данных
- Интерпретации результатов для нетехнической аудитории

**BERTopic рекомендуется для:**
- Детального анализа специфических тем
- Работы с данными, где важна семантическая близость
- Исследовательских задач с толерантностью к outliers

### 6.2 Направления дальнейших исследований

1. Исследование гибридных подходов, объединяющих преимущества обеих методологий
2. Разработка специализированных метрик для оценки BERTopic моделей
3. Анализ временной динамики тем в лонгитюдных данных
4. Адаптация методов для других языков и доменов

## Список литературы

1. Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.

2. Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint arXiv:2203.05794.

3. Röder, M., Both, A., & Hinneburg, A. (2015). Exploring the space of topic coherence measures. Proceedings of the eighth ACM international conference on Web search and data mining.

4. Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence embeddings using Siamese BERT-networks. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.

## Приложения

### Приложение A: Конфигурация эксперимента
- **Операционная система:** Windows 10
- **Python:** 3.10
- **Основные библиотеки:** 
  - gensim==4.3.3
  - bertopic==0.17.0
  - sentence-transformers==4.1.0
  - scikit-learn==1.6.1

### Приложение B: Репозиторий кода
Полный код исследования доступен в репозитории проекта, включая:
- Скрипты предобработки данных
- Реализацию моделей
- Метрики оценки
- Скрипты визуализации

---

*Дата создания отчета: 19 июня 2025*  
*Версия: 1.0* 